{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "1. Information Theory, Inference and Learning Algorithms, David J. C. MacKay, David J. C. Mac Kay, Cambridge University Press, 2003\n",
    "2. Bayesian Reasoning and Machine Learning, David Barber, Cambridge University Press, 2012\n",
    "3. Pattern Recognition and Machine Learning, Christopher M. Bishop, Springer New York, 2016\n",
    "4. Variational Inference: A Review for Statisticians, David M. Blei, Alp Kucukelbir, Jon D. McAuliffe, 2018\n",
    " . High-Level Explanation of Variational Inference, Jason Eisner, 2011\n",
    " \n",
    "\n",
    "The work presented here is not original neither I claim it my own. The text and the maths have been extracted and/or copied and compiled from the sources mentioned above. \n",
    "\n",
    "**\"Render unto Caesar the things that are Caesar's\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bibliography that I have listed is ranked by the most early work that I read. Some other people might prefer to start differently. I have also tried to solve the exercises from these books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Variational Inference\n",
    "\n",
    "Variational Inference (VI) methods are used to identify complex distributions as an alternative to the computationally costly (but very accurate) Markov chain Monte Carlo (MCMC) sampling. In general, MCMC provide guarantees of producing (asymptotically) exact samples from the target density [4, p.3]. In VI case, we use a less complex but still flexible **family** of distributions to approximate an intractable distribution.\n",
    "\n",
    "For business and academic projects, where Bayesian models are fitted on very large datasets (Big Data applications), approximate inference methods are a very efficient and optimised way of infering quite accurate results.\n",
    "\n",
    "\n",
    "Assume that we want to use VI on a Bayesian regression, let $x$ be the independent variables with $X$ their matrix form and $y$ the continuous responce variable. The coefficients $\\beta$ of the model follow a complex multimodal distribution. \n",
    "\n",
    "$$\n",
    "y = \\mathcal{N}(\\beta^{T}X, \\sigma^{2})\n",
    "$$\n",
    "\n",
    "in Bayesian form, we want to infer the posterior of the model, in other words the distribution of the coefficients\n",
    "\n",
    "$$\n",
    "p(\\beta|y, X) = \\frac{p(y|X, \\beta)p(\\beta|X)}{p(y)}\n",
    "$$\n",
    "\n",
    "The complexity of the posterior distribution means that we have two ways to compute the results\n",
    "\n",
    "1. Use MCMC sampling methods (Gibb's, Hamiltonian)\n",
    "2. Use approximate methods\n",
    "\n",
    "To use approximate inference we follow the steps, that are similar to [1, 2, 4]\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(q(\\beta)||p(\\beta|y, X)) &= \\int d\\beta q(\\beta) log \\frac{q(\\beta)}{p(\\beta|y, X)}\\\\\n",
    "&= \\int d\\beta q(\\beta) log q(\\beta) - \\int d\\beta q(\\beta) log p(\\beta|y, X) \\\\\n",
    "&= \\int d\\beta q(\\beta) log q(\\beta) - \\int d\\beta q(\\beta) log \\frac{p(y|\\beta, X)p(\\beta|X)}{p(y)} \\\\\n",
    "&= \\int d\\beta q(\\beta) log q(\\beta) - \\int d\\beta q(\\beta) log p(y|\\beta, X)p(\\beta|X) + log p(y) \\\\\n",
    "&= \\int d\\beta q(\\beta) log q(\\beta) - \\int d\\beta q(\\beta) log p(y|\\beta, X) - \\int q(\\beta) log p(\\beta|X) + log p(y) \\\\\n",
    "&= \\int d\\beta q(\\beta) log q(\\beta) - \\int d\\beta q(\\beta) log p(\\beta|X) - \\int q(\\beta) log p(y|\\beta, X) + log p(y) \\\\\n",
    "&= \\int d\\beta q(\\beta) log \\frac{q(\\beta)}{p(\\beta|X)} - \\int d\\beta q(\\beta) log p(y|\\beta, X) + log p(y) \\\\\n",
    "&= KL(q(\\beta)||p(\\beta|X)) - \\mathbb{E}_{q}(log p(y|\\beta, X)) + log p(y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the RHS of the equation shows the entropy, the average energy and the evidence (marginal of $y$).\n",
    "By setting the lower bound (and change the signs because we move the $log p(y)$ on the LHS) as\n",
    "\n",
    "$$\n",
    "ELBO(q) = \\mathbb{E}_{q}(log p(y|\\beta, X)) - KL(q(\\beta)||p(\\beta|X))\n",
    "$$\n",
    "\n",
    "and the previous formula becomes\n",
    "\n",
    "$$\n",
    "log p(y) = ELBO(q) + KL(q(\\beta)||p(\\beta|y, X))\n",
    "$$\n",
    "\n",
    "which is similar to the formula that Bishop has in his book [3, p.463]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 2, 1000).reshape((-1,10))\n",
    "betas = [0.2, 1.4, 0.1, -0.9, 1.2, 0.7, -0.5, 1.7, 1.1, -0.4]\n",
    "y = np.matmul(x, betas) + np.random.normal(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD\n",
    "class BayesianRegression:\n",
    "    '''\n",
    "    A class that fits a Bayesian regression model on continuous data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.author = \"Leo Ardon, Francois Buet Golfouse, Georgios Papadopoulos\"\n",
    "        self.description = \"This function fits a Bayesian model on the data\"\n",
    "        \n",
    "    def fit(self):\n",
    "        return np.matmul(np.linalg.inv(np.matmul(self.x.T, self.x)), np.matmul(self.x.T, self.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianRegression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26546431,  1.4391956 ,  0.1040395 , -0.97480022,  1.17989222,\n",
       "        0.64778338, -0.47549614,  1.7215252 ,  1.08644829, -0.44385854])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
